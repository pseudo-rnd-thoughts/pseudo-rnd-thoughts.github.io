---
layout: post
title: "Generalized Advantage Estimation (GAE) Explained"
date: 2026-01-31
math: true
excerpt: "Generalised Advantage Estimation (GAE) is a critical component of PPO but how does GAE work? What does it achieve?"
---

<section id="policy-gradient">
    <h2>How Do We Improve a Policy?</h2>
    <p>
        Suppose we have a policy and roll it out, collecting states, actions, and rewards. Some actions worked out, increasing our return, and others didn't. To improve, we want to increase the likeliness of taking the "good" actions in the future, and the "bad" actions less likely. But how do we turn that into a gradient we can backpropagate?
    </p>
    <p>
        The Policy Gradient Theorem gives us the answer. For each action in our rollout, we multiply two things together: the direction that makes the action more likely, and a score for how good the action was. Summing over all timesteps gives us the gradient of our expected return:
    </p>
    <div class="math-block">
        \[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) \cdot A_t \right]\]
    </div>
    <p>
        The first term, \(\nabla_\theta \log \pi_\theta(a_t | s_t)\), points in the direction that increases the probability of action \(a_t\). The second term, \(A_t\), is the advantage representing how much better this action was compared to what the policy would normally do from that state:
    </p>
    <div class="math-block highlight">
        \[A(s_t, a_t) = Q(s_t, a_t) - V(s_t)\]
    </div>
    <p>
        \(Q(s_t, a_t)\) is the expected return from taking action \(a_t\) in state \(s_t\), and \(V(s_t) = \mathbb{E}_{a \sim \pi}[Q(s_t, a)]\) is the expected return averaged over all actions the policy might take. Positive advantage means "do more of this", negative means "do less".
    </p>
</section>

<section id="bias-variance">
    <h2>The Estimation Problem: Bias vs Variance</h2>
    <p>
        Computing the advantage requires comparing what rewards we actually received in the rollout (\(\sum_{k=0}^{\infty} \gamma^k r_{t+k}\)) against what rewards we expected to receive (\(V(s_t)\)). The quality of our advantage estimate depends entirely on how we estimate this expected value, and there are two classical approaches:
    </p>
    <ul>
        <li>
            <span class="text-highlight">Monte Carlo</span> only uses the rewards collected in the rollout for the Q-value target ensuring that it correct on average (i.e., unbiased) but each rollout's rewards are noisy (from the randomness in the policy and environment) means that different rollouts give very different totals, resulting in high variance.
        </li>
        <li>
            <span class="text-highlight">Temporal Difference (TD) Error</span> replaces the future rewards after \(t+1\) with estimate of their expected sum \(V(s_{t+1})\). This compresses all that noise across rollouts into a stable, deterministic output, giving low variance. But if the learned \(V\) is inaccurate or systematically shifted from the correct scale it will be <strong>biased</strong>.
        </li>
    </ul>

    <table class="comparison-table">
        <thead>
            <tr>
                <th>Method</th>
                <th>Formula</th>
                <th>Bias</th>
                <th>Variance</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Monte Carlo</strong></td>
                <td>\(A_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k} - V(s_t)\)</td>
                <td><span class="badge badge-low">Unbiased</span></td>
                <td><span class="badge badge-high">High</span></td>
            </tr>
            <tr>
                <td><strong>TD Error</strong></td>
                <td>\(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\)</td>
                <td><span class="badge badge-high">Biased</span></td>
                <td><span class="badge badge-low">Low</span></td>
            </tr>
        </tbody>
    </table>
</section>

<section id="gae">
    <h2>GAE: Balancing the Tradeoff</h2>
    <p>
        <strong>Generalized Advantage Estimation (GAE)</strong>, introduced by Schulman et al. (2016), provides a solution to the trade-off between the variance and bias of Monte Carlo and TD Error. It is parameterized by \(\lambda \in [0, 1]\) which controls how much we lean towards TD (low variance, higher bias) or Monte Carlo (unbiased, higher variance):
    </p>

    <div class="lambda-spectrum">
        <div class="spectrum-end left">
            <h4>\(\lambda = 0\)</h4>
            <div class="formula">\(A_t = \delta_t\)</div>
            <p>Pure TD error<br><span class="badge badge-low">Low variance</span> <span class="badge badge-high">High bias</span></p>
        </div>
        <div class="spectrum-middle">
            <div class="spectrum-arrow">\(\longleftrightarrow\)</div>
            <p style="margin: 8px 0 0; font-size: 0.85rem;">GAE(\(\lambda\))</p>
        </div>
        <div class="spectrum-end right">
            <h4>\(\lambda = 1\)</h4>
            <div class="formula">\(A_t = \sum_k \gamma^k r_{t+k} - V(s_t)\)</div>
            <p>Monte Carlo-like<br><span class="badge badge-high">High variance</span> <span class="badge badge-low">Low bias</span></p>
        </div>
    </div>

    <p>
        The key insight of GAE is to combine TD errors from multiple time horizons. Instead of using just the immediate 1-step TD error (which is biased but stable) or the full Monte Carlo return (which is unbiased but noisy), GAE takes a weighted average of all future TD errors. The parameter \(\lambda\) controls how quickly we discount future errors: with \(\lambda = 0\), we only use the immediate error, while \(\lambda = 1\) uses all future errors equally, recovering something close to Monte Carlo.
    </p>

    <h3>GAE Formula</h3>
    <p>
        GAE takes an <strong>exponentially-weighted sum of TD errors</strong> \(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\), with weights decaying by \(\gamma\lambda\) at each step:
    </p>
    <div class="math-block highlight">
        \[A_t^{\text{GAE}(\gamma, \lambda)} = \sum_{k=0}^{\infty} (\gamma \lambda)^k \delta_{t+k}\]
    </div>
    <p>
        In practice, we compute this efficiently using a backward pass through the trajectory. Starting from the last timestep (where \(A_T = \delta_T\)) and working backwards:
    </p>
    <div class="math-block">
        \[A_t = \delta_t + \gamma \lambda \cdot A_{t+1}\]
    </div>
    <p>
        Each advantage incorporates the TD error at that step plus a discounted, decayed version of future advantages. This is exactly what you saw in the visualisation above.
    </p>
</section>

<!-- Interactive visualisation -->
<section class="viz-section" id="visualisation">
    <h2>Interactive Rollout Visualisation</h2>
    <p class="viz-description">Watch the backward computation unfold step by step. Adjust \(\gamma\) and \(\lambda\) to see how they affect the advantage estimates.</p>

    <div class="controls">
        <div class="control-group">
            <label>Discount \(\gamma\)</label>
            <div class="control-value" id="gamma-display">0.99</div>
            <div class="slider-container">
                <input type="range" id="gamma-slider" min="0.9" max="1" step="0.01" value="0.99">
            </div>
        </div>
        <div class="control-group">
            <label>Lambda \(\lambda\)</label>
            <div class="control-value" id="lambda-display">0.95</div>
            <div class="slider-container">
                <input type="range" id="lambda-slider" min="0" max="1" step="0.05" value="0.95">
            </div>
        </div>
        <div class="control-group">
            <label>Actions</label>
            <div class="button-group">
                <button class="secondary" id="reset-btn">Reset</button>
                <button class="primary" id="animate-btn">Animate</button>
            </div>
        </div>
    </div>

    <div class="rollout-container">
        <div class="rollout" id="rollout">
            <!-- Generated by JavaScript -->
        </div>
        <div class="computation-rows" id="computation-rows">
            <!-- Generated by JavaScript -->
        </div>
    </div>

    <div class="step-log" id="step-log">
        <div class="step-log-title">Computation Steps</div>
        <div id="log-entries">
            <div class="log-entry visible">
                <span class="log-step">Step 1:</span>
                <span class="log-formula">Compute TD errors \(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\)</span>
            </div>
            <div class="log-entry visible">
                <span class="log-step">Step 2:</span>
                <span class="log-formula">Compute GAE backward: \(A_t = \delta_t + \gamma\lambda A_{t+1}\)</span>
            </div>
        </div>
    </div>
</section>

<section id="implementation">
    <h2>Implementation</h2>
    <p>
        Here's how GAE is typically implemented in practice. The key insight is computing advantages in reverse order. We also need to handle episode boundaries correctly, since a batch of experience may span multiple episodes.
    </p>
    <p>
        Episodes can end in two ways: <strong>termination</strong> (a true terminal state, e.g. death or reaching the goal) or <strong>truncation</strong> (a time limit was hit). These require different treatment:
    </p>
    <ul>
        <li>On <strong>termination</strong>, there is no next state, so the bootstrap value \(V(s_{t+1})\) should be zero.</li>
        <li>On <strong>truncation</strong>, the agent could have continued, so we still bootstrap with \(V(s_{t+1})\).</li>
    </ul>
    <p>
        In both cases, we reset the backward pass at the episode boundary so that advantages from one episode don't bleed into another.
    </p>
    <pre><code><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-keyword">def</span> compute_gae(rewards, values, terminated, truncated, gamma, lambda_):
    <span class="code-comment"># values has length T+1 (includes bootstrap value)</span>
    <span class="code-comment"># rewards, terminated, truncated have length T</span>
    T = len(rewards)
    advantages = np.zeros(T)
    gae = <span class="code-number">0</span>

    <span class="code-comment"># Terminated: no future value. Truncated: bootstrap as normal.</span>
    next_values = np.where(terminated, <span class="code-number">0</span>, values[<span class="code-number">1</span>:])
    deltas = rewards + gamma * next_values - values[:-<span class="code-number">1</span>]

    <span class="code-comment"># Reset at any episode boundary (terminated or truncated)</span>
    dones = terminated | truncated

    <span class="code-keyword">for</span> t <span class="code-keyword">in</span> reversed(range(T)):
        gae = deltas[t] + gamma * lambda_ * (~dones[t]) * gae
        advantages[t] = gae

    <span class="code-keyword">return</span> advantages</code></pre>

</section>

<section id="practical">
    <h2>Practical Considerations</h2>

    <h3>Choosing \(\lambda\)</h3>
    <p>
        In practice, \(\lambda\) values between <strong>0.9 and 0.99</strong> work well for most problems. The PPO paper uses \(\lambda = 0.95\) as a default. Higher values are better when your value function is inaccurate; lower values help when you have a good value function and want to reduce variance.
    </p>

    <h3>Interaction with \(\gamma\)</h3>
    <p>
        The effective discount for the advantage weights is \(\gamma\lambda\). Even with \(\lambda = 1\), using \(\gamma < 1\) still provides some exponential decay. Common choices are \(\gamma = 0.99\) with \(\lambda = 0.95\), giving an effective decay of \(0.99 \times 0.95 = 0.9405\).
    </p>

    <div class="insight-box">
        <h4>Why GAE Works</h4>
        <p>
            GAE succeeds because it lets you control how much you trust your value function versus the raw sampled returns. A lower \(\lambda\) leans on \(V\) more (stable but potentially biased); a higher \(\lambda\) relies on actual rewards (unbiased but noisy). Tuning \(\lambda\) finds the right balance for your specific problem.
        </p>
    </div>
</section>

<section id="references">
    <h2>References</h2>
    <p>
        Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2016). <em>High-Dimensional Continuous Control Using Generalized Advantage Estimation.</em> ICLR 2016.
    </p>
</section>

<script>
    // Rollout data
    const rolloutData = {
        states: ['s₀', 's₁', 's₂', 's₃', 's₄', 's₅'],
        values: [2.5, 3.1, 2.8, 4.2, 3.0, 0], // V(s_5) = 0 (terminal)
        rewards: [1.0, -0.5, 2.0, 0.5, -1.0]  // r_0 to r_4
    };

    let gamma = 0.99;
    let lambda = 0.95;
    let animationInProgress = false;

    // DOM elements
    const gammaSlider = document.getElementById('gamma-slider');
    const lambdaSlider = document.getElementById('lambda-slider');
    const gammaDisplay = document.getElementById('gamma-display');
    const lambdaDisplay = document.getElementById('lambda-display');
    const rolloutContainer = document.getElementById('rollout');
    const computationRows = document.getElementById('computation-rows');
    const logEntries = document.getElementById('log-entries');
    const animateBtn = document.getElementById('animate-btn');
    const resetBtn = document.getElementById('reset-btn');

    // Compute TD errors
    function computeTDErrors() {
        const deltas = [];
        for (let t = 0; t < rolloutData.rewards.length; t++) {
            const delta = rolloutData.rewards[t] + gamma * rolloutData.values[t + 1] - rolloutData.values[t];
            deltas.push(delta);
        }
        return deltas;
    }

    // Compute GAE
    function computeGAE(deltas) {
        const advantages = new Array(deltas.length).fill(0);
        let gae = 0;
        for (let t = deltas.length - 1; t >= 0; t--) {
            gae = deltas[t] + gamma * lambda * gae;
            advantages[t] = gae;
        }
        return advantages;
    }

    // Render rollout visualisation
    function renderRollout() {
        let html = '';

        for (let t = 0; t < rolloutData.states.length; t++) {
            // State node
            html += `
                <div class="timestep" data-t="${t}">
                    <div class="state-node" id="state-${t}">${rolloutData.states[t]}</div>
                    <div class="value-label">V(${rolloutData.states[t]})</div>
                    <div class="value">${rolloutData.values[t].toFixed(1)}</div>
                </div>
            `;

            // Transition arrow and reward (if not last state)
            if (t < rolloutData.rewards.length) {
                const reward = rolloutData.rewards[t];
                const rewardClass = reward >= 0 ? 'positive' : 'negative';
                html += `
                    <div class="transition">
                        <div class="arrow"></div>
                        <div class="reward ${rewardClass}">${reward >= 0 ? '+' : ''}${reward.toFixed(1)}</div>
                        <div class="reward-label">r${t}</div>
                    </div>
                `;
            }
        }

        rolloutContainer.innerHTML = html;
    }

    // Render computation rows
    function renderComputationRows() {
        const deltas = computeTDErrors();
        const advantages = computeGAE(deltas);

        let html = '';

        // TD errors row
        html += '<div class="computation-row" id="td-row"><div class="label">TD Error</div>';
        for (let t = 0; t < deltas.length; t++) {
            html += `
                <div class="computation-cell">
                    <span class="symbol">δ${t}</span>
                    <span class="value td" id="td-${t}">${deltas[t].toFixed(3)}</span>
                </div>
            `;
            if (t < deltas.length - 1) {
                html += '<div class="spacer-cell"></div>';
            }
        }
        html += '</div>';

        // GAE row
        html += '<div class="computation-row" id="gae-row"><div class="label">GAE</div>';
        for (let t = 0; t < advantages.length; t++) {
            const gaeClass = advantages[t] >= 0 ? 'gae-positive' : 'gae-negative';
            html += `
                <div class="computation-cell">
                    <span class="symbol">A${t}</span>
                    <span class="value ${gaeClass}" id="gae-${t}">${advantages[t].toFixed(3)}</span>
                </div>
            `;
            if (t < advantages.length - 1) {
                html += '<div class="spacer-cell"></div>';
            }
        }
        html += '</div>';

        computationRows.innerHTML = html;
    }

    // Animation
    async function animate() {
        if (animationInProgress) return;
        animationInProgress = true;
        animateBtn.disabled = true;
        resetBtn.disabled = true;

        const deltas = computeTDErrors();
        const advantages = computeGAE(deltas);

        // Reset visibility
        document.querySelectorAll('.computation-cell .value').forEach(el => {
            el.classList.remove('visible');
        });
        document.querySelectorAll('.state-node').forEach(el => {
            el.classList.remove('active', 'computed');
        });

        // Clear previous log entries (keep the first two)
        const existingEntries = logEntries.querySelectorAll('.log-entry');
        existingEntries.forEach((entry, i) => {
            if (i >= 2) entry.remove();
        });

        // Animate TD errors (forward)
        for (let t = 0; t < deltas.length; t++) {
            const stateNode = document.getElementById(`state-${t}`);
            stateNode.classList.add('active');

            await sleep(300);

            const tdValue = document.getElementById(`td-${t}`);
            tdValue.classList.add('visible');

            stateNode.classList.remove('active');
            await sleep(200);
        }

        await sleep(500);

        // Animate GAE (backward)
        let gae = 0;
        for (let t = deltas.length - 1; t >= 0; t--) {
            const stateNode = document.getElementById(`state-${t}`);
            stateNode.classList.add('active');

            // Add log entry
            const prevGae = gae;
            gae = deltas[t] + gamma * lambda * gae;

            let logHtml;
            if (t === deltas.length - 1) {
                logHtml = `<span class="log-step">t=${t}:</span> <span class="log-formula">A${t} = δ${t} = </span><span class="log-result">${advantages[t].toFixed(3)}</span>`;
            } else {
                logHtml = `<span class="log-step">t=${t}:</span> <span class="log-formula">A${t} = ${deltas[t].toFixed(3)} + ${gamma}×${lambda}×${prevGae.toFixed(3)} = </span><span class="log-result">${advantages[t].toFixed(3)}</span>`;
            }

            const logEntry = document.createElement('div');
            logEntry.className = 'log-entry';
            logEntry.innerHTML = logHtml;
            logEntries.appendChild(logEntry);

            await sleep(100);
            logEntry.classList.add('visible');

            await sleep(300);

            const gaeValue = document.getElementById(`gae-${t}`);
            gaeValue.classList.add('visible');

            stateNode.classList.remove('active');
            stateNode.classList.add('computed');

            await sleep(400);
        }

        animationInProgress = false;
        animateBtn.disabled = false;
        resetBtn.disabled = false;
    }

    // Reset visualisation
    function reset() {
        renderRollout();
        renderComputationRows();

        // Reset log
        logEntries.innerHTML = `
            <div class="log-entry visible">
                <span class="log-step">Step 1:</span>
                <span class="log-formula">Compute TD errors δₜ = rₜ + γV(sₜ₊₁) - V(sₜ)</span>
            </div>
            <div class="log-entry visible">
                <span class="log-step">Step 2:</span>
                <span class="log-formula">Compute GAE backward: Aₜ = δₜ + γλAₜ₊₁</span>
            </div>
        `;

        // Show all values immediately
        document.querySelectorAll('.computation-cell .value').forEach(el => {
            el.classList.add('visible');
        });
    }

    function sleep(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }

    // Event listeners
    gammaSlider.addEventListener('input', (e) => {
        gamma = parseFloat(e.target.value);
        gammaDisplay.textContent = gamma.toFixed(2);
        renderComputationRows();
        document.querySelectorAll('.computation-cell .value').forEach(el => {
            el.classList.add('visible');
        });
    });

    lambdaSlider.addEventListener('input', (e) => {
        lambda = parseFloat(e.target.value);
        lambdaDisplay.textContent = lambda.toFixed(2);
        renderComputationRows();
        document.querySelectorAll('.computation-cell .value').forEach(el => {
            el.classList.add('visible');
        });
    });

    animateBtn.addEventListener('click', animate);
    resetBtn.addEventListener('click', reset);

    // Initial render
    reset();
</script>
