---
layout: post
title: "How does Retrace fix Off-Policyness?"
date: 2026-02-12
excerpt: "When your RL agent trains on data from an old policy, the value estimates go wrong. Retrace was designed to address this problem."
math: true
---

<style>
    /* Visualization specific styles */
    .policy-indicator {
        display: inline-flex;
        align-items: center;
        gap: 6px;
        padding: 4px 10px;
        border-radius: 6px;
        font-size: 0.85rem;
        font-family: 'JetBrains Mono', monospace;
    }

    .policy-target {
        background: rgba(78, 205, 196, 0.15);
        border: 1px solid var(--accent-primary);
    }

    .policy-behavior {
        background: rgba(255, 166, 87, 0.15);
        border: 1px solid var(--accent-orange);
    }

    .policy-legend {
        display: flex;
        gap: 24px;
        margin-bottom: 24px;
        flex-wrap: wrap;
    }

    .ratio-display {
        display: flex;
        flex-direction: column;
        align-items: center;
        font-family: 'JetBrains Mono', monospace;
    }

    .ratio-fraction {
        display: flex;
        flex-direction: column;
        align-items: center;
        font-size: 0.8rem;
    }

    .ratio-numerator {
        color: var(--accent-primary);
        border-bottom: 1px solid var(--text-muted);
        padding-bottom: 2px;
    }

    .ratio-denominator {
        color: var(--accent-orange);
        padding-top: 2px;
    }

    .ratio-value {
        font-size: 1.1rem;
        font-weight: 600;
        margin-top: 4px;
    }

    .ratio-value.high {
        color: var(--accent-red);
    }

    .ratio-value.normal {
        color: var(--text-primary);
    }

    .ratio-value.low {
        color: var(--accent-blue);
    }

    .truncated-value {
        color: var(--accent-green) !important;
    }

    .action-label {
        font-size: 0.75rem;
        color: var(--text-muted);
        margin-top: 4px;
    }

    /* Algorithm selector tabs */
    .algorithm-tabs {
        display: flex;
        gap: 8px;
        margin-bottom: 24px;
        flex-wrap: wrap;
    }

    .algorithm-tab {
        padding: 10px 20px;
        border: 1px solid var(--accent-border);
        border-radius: 8px;
        background: var(--bg-tertiary);
        color: var(--text-secondary);
        cursor: pointer;
        transition: all 0.2s;
        font-size: 0.9rem;
    }

    .algorithm-tab:hover {
        border-color: var(--accent-primary);
        color: var(--text-primary);
    }

    .algorithm-tab.active {
        background: var(--accent-primary);
        color: var(--bg-primary);
        border-color: var(--accent-primary);
    }

    /* Extended transition for policy probs */
    .transition-extended {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: flex-start;
        min-width: 100px;
        padding-top: 23px;
    }

    .prob-row {
        display: flex;
        gap: 12px;
        margin-top: 8px;
        font-size: 0.75rem;
    }

    .prob-item {
        display: flex;
        flex-direction: column;
        align-items: center;
    }

    .prob-label {
        color: var(--text-muted);
    }

    .prob-value-target {
        color: var(--accent-primary);
        font-family: 'JetBrains Mono', monospace;
    }

    .prob-value-behavior {
        color: var(--accent-orange);
        font-family: 'JetBrains Mono', monospace;
    }

    /* Product visualization */
    .product-chain {
        display: flex;
        align-items: center;
        gap: 8px;
        font-family: 'JetBrains Mono', monospace;
        flex-wrap: wrap;
        justify-content: center;
        padding: 16px;
        background: var(--bg-primary);
        border-radius: 8px;
        margin: 16px 0;
    }

    .product-term {
        padding: 4px 8px;
        border-radius: 4px;
        background: var(--bg-tertiary);
    }

    .product-term.included {
        border: 1px solid var(--accent-primary);
    }

    .product-term.excluded {
        opacity: 0.4;
        text-decoration: line-through;
    }

    .product-operator {
        color: var(--text-muted);
    }

    .product-result {
        color: var(--accent-green);
        font-weight: 600;
        padding-left: 8px;
        border-left: 2px solid var(--accent-border);
        margin-left: 8px;
    }

    /* Variance indicator */
    .variance-meter {
        display: flex;
        align-items: center;
        gap: 12px;
        margin: 16px 0;
        padding: 12px 16px;
        background: var(--bg-primary);
        border-radius: 8px;
    }

    .variance-label {
        font-size: 0.85rem;
        color: var(--text-muted);
        min-width: 80px;
    }

    .variance-bar {
        flex: 1;
        height: 8px;
        background: var(--bg-tertiary);
        border-radius: 4px;
        overflow: hidden;
    }

    .variance-fill {
        height: 100%;
        border-radius: 4px;
        transition: width 0.5s ease-out;
    }

    .variance-fill.low {
        background: var(--accent-green);
    }

    .variance-fill.medium {
        background: var(--accent-orange);
    }

    .variance-fill.high {
        background: var(--accent-red);
    }

    .variance-value {
        font-family: 'JetBrains Mono', monospace;
        font-size: 0.9rem;
        min-width: 60px;
        text-align: right;
    }

    /* Formula highlight box */
    .formula-box {
        background: var(--bg-primary);
        border: 1px solid var(--accent-border);
        border-radius: 8px;
        padding: 16px 20px;
        margin: 16px 0;
    }

    .formula-box.active {
        border-color: var(--accent-primary);
        box-shadow: 0 0 15px var(--accent-primary-dim);
    }

    .formula-box .formula-name {
        font-size: 0.8rem;
        color: var(--text-muted);
        text-transform: uppercase;
        letter-spacing: 0.5px;
        margin-bottom: 8px;
    }

    /* Trace coefficient row */
    .trace-row {
        display: flex;
        align-items: center;
        gap: 0;
        min-width: max-content;
        padding: 0 20px;
        margin-bottom: 16px;
    }

    .trace-cell {
        min-width: 120px;
        display: flex;
        flex-direction: column;
        align-items: center;
    }

    .trace-cell .trace-symbol {
        font-size: 0.85rem;
        color: var(--text-muted);
    }

    .trace-cell .trace-value {
        font-family: 'JetBrains Mono', monospace;
        font-size: 1rem;
        font-weight: 600;
        margin-top: 4px;
        transition: all 0.3s;
    }

    .trace-cell .trace-value.truncated {
        color: var(--accent-green);
    }

    .trace-cell .trace-value.full {
        color: var(--accent-red);
    }

    .trace-cell .truncate-indicator {
        font-size: 0.7rem;
        color: var(--accent-green);
        margin-top: 2px;
    }
</style>

<section id="introduction">
    <h2>Why Off-Policy Correction Matters</h2>
    <p>
        Consider a typical deep RL training loop. Your agent collects a batch of experience using its current policy, stores it in a replay buffer, then updates the policy with gradient descent. By the time you sample that experience again for a second training step, the policy has already changed. The data was collected by an older version of the policy, but you're trying to improve the current one.
    </p>
    <p>
        This gap between the <strong>data-collecting policy</strong> (called the <strong>behavior policy</strong>, μ) and the <strong>policy being improved</strong> (the <strong>target policy</strong>, π) is the off-policy problem. It shows up everywhere: experience replay in DQN, distributed training where actors lag behind the learner (as in IMPALA), or learning from demonstrations and historical data.
    </p>
    <p>
        So what goes wrong? The outcomes in our replay buffer reflect how μ behaved, not how π would behave. If μ frequently took some action that π would rarely choose, our data over-represents that action's outcomes. If π strongly prefers an action that μ rarely took, those outcomes are underrepresented.
    </p>
    <p>
        Without correction, our value estimates inherit this mismatch: they become biased towards what the old policy did, not what the current policy would do. In the worst case, this bias compounds over updates and learning diverges entirely.
    </p>

    <div class="insight-box">
        <h4>The Core Problem</h4>
        <p>Data in the replay buffer was generated by a past version of the policy. Using it directly to update the current policy gives biased value estimates because the distribution of actions (and therefore trajectories) no longer matches.</p>
    </div>
</section>

<section id="importance-sampling">
    <h2>Importance Sampling: The Basic Fix</h2>
    <p>
        The standard tool for correcting this distribution mismatch is <strong>importance sampling</strong>. The idea is simple: if the target policy is twice as likely to take some action compared to the behavior policy, we should count that transition twice as much. If it's half as likely, we should halve its contribution.
    </p>
    <p>
        We capture this with the <strong>importance sampling ratio</strong>, which compares the probability each policy assigns to the action that was actually taken in state \(s_t\):
    </p>

    <div class="math-block highlight">
        $$\rho_t = \frac{\pi(a_t | s_t)}{\mu(a_t | s_t)}$$
    </div>

    <p>
        Here \(\pi(a_t | s_t)\) is the probability the target policy assigns to action \(a_t\) in state \(s_t\), and \(\mu(a_t | s_t)\) is the probability the behavior policy assigned to it. The ratio tells us how to reweight each transition:
    </p>
    <ul>
        <li><strong>ρ > 1</strong>: The target policy favors this action more than the behavior policy did, so we upweight this transition</li>
        <li><strong>ρ < 1</strong>: The target policy is less likely to take this action, so we downweight it</li>
        <li><strong>ρ = 1</strong>: Both policies agree on this action, so no correction is needed</li>
    </ul>
    <p>
        With this ratio in hand, we can correct off-policy data. But how we apply it makes a big difference, and that's where things get interesting.
    </p>
</section>

<section id="naive-approach">
    <h2>Why N-Step Returns Alone Don't Work</h2>
    <p>
        A natural first idea is to use multiple steps of real rewards before bootstrapping from our value function. Using more real rewards means we rely less on a potentially inaccurate value estimate, reducing bias. The n-step return looks like:
    </p>
    <div class="math-block">
        $$G_t^{(n)} = \sum_{k=0}^{n-1} \gamma^k r_{t+k} + \gamma^n V(s_{t+n})$$
    </div>
    <p>
        But those n steps of rewards came from the behavior policy μ. To correct for this, we multiply by the product of importance sampling ratios across all n steps:
    </p>
    <div class="math-block highlight">
        $$G_t^{IS} = \left(\prod_{k=0}^{n-1} \rho_{t+k}\right) \cdot G_t^{(n)}$$
    </div>
    <p>
        Each individual ratio might be modest, but they multiply together. If the policies have diverged so that each ρ averages around 2, after just 10 steps the correction factor is 2<sup>10</sup> = 1024. A single trajectory can dominate the entire update, causing massive variance that destabilises learning.
    </p>
    <p>
        This creates a frustrating dilemma. Using fewer steps keeps the IS product small (low variance) but forces us to trust a possibly wrong value function (high bias). Using more steps reduces that bias but makes the IS product explode (high variance). We need a way to use multi-step returns without the variance blowing up.
    </p>
</section>

<section id="retrace">
    <h2>Retrace(λ): Safe Off-Policy Learning</h2>
    <p>
        Retrace breaks out of this dilemma with one key modification: instead of multiplying raw importance ratios, it <strong>truncates</strong> each ratio before multiplying. It defines a <strong>trace coefficient</strong>:
    </p>
    <div class="math-block">
        $$c_t = \lambda \cdot \min(1, \rho_t)$$
    </div>
    <p>
        The \(\min(1, \rho_t)\) caps each ratio at 1, and the λ parameter (between 0 and 1) provides additional decay. Since every \(c_t\) is at most λ, the product of trace coefficients can only shrink over time. No matter how different the two policies are, the correction weights stay bounded.
    </p>
    <p>
        Retrace uses these trace coefficients to weight TD errors across multiple steps. The target for each timestep is:
    </p>
    <div class="math-block highlight">
        $$G_t^{ret} = Q(s_t, a_t) + \sum_{k=0}^{T-t-1} \gamma^k \left(\prod_{j=1}^{k} c_{t+j}\right) \delta_{t+k}$$
    </div>
    <p>
        where \(\delta_t = r_t + \gamma V(s_{t+1}) - Q(s_t, a_t)\) is the TD error at step t. In practice, this is computed efficiently with a backward pass using the recursive form:
    </p>
    <div class="math-block">
        $$G_t^{ret} = r_t + \gamma \left[ c_{t+1}(G_{t+1}^{ret} - Q(s_{t+1}, a_{t+1})) + V(s_{t+1}) \right]$$
    </div>
</section>

<!-- Interactive Visualization -->
<section class="viz-section" id="visualization">
    <h2>Interactive Visualization</h2>
    <p class="viz-description">
        Explore how different algorithms handle off-policy data. Adjust the policy divergence to see how importance ratios affect each method.
    </p>

    <div class="policy-legend">
        <div class="policy-indicator policy-target">
            <span>π</span> Target Policy
        </div>
        <div class="policy-indicator policy-behavior">
            <span>μ</span> Behavior Policy
        </div>
    </div>

    <div class="algorithm-tabs">
        <button class="algorithm-tab active" data-algo="td">1-Step TD</button>
        <button class="algorithm-tab" data-algo="nstep">N-Step</button>
        <button class="algorithm-tab" data-algo="retrace">Retrace(λ)</button>
        <button class="algorithm-tab" data-algo="vtrace">V-trace</button>
    </div>

    <div class="controls">
        <div class="control-group">
            <label>Policy Divergence</label>
            <div class="control-value" id="divergence-display">Medium</div>
            <div class="slider-container">
                <input type="range" id="divergence-slider" min="0" max="2" step="1" value="1">
            </div>
        </div>
        <div class="control-group" id="lambda-control">
            <label>Lambda λ</label>
            <div class="control-value" id="lambda-display">0.95</div>
            <div class="slider-container">
                <input type="range" id="lambda-slider" min="0" max="1" step="0.05" value="0.95">
            </div>
        </div>
        <div class="control-group">
            <label>Discount γ</label>
            <div class="control-value" id="gamma-display">0.99</div>
            <div class="slider-container">
                <input type="range" id="gamma-slider" min="0.9" max="1" step="0.01" value="0.99">
            </div>
        </div>
        <div class="control-group">
            <label>Actions</label>
            <div class="button-group">
                <button class="secondary" id="reset-btn">Reset</button>
                <button class="primary" id="animate-btn">Animate</button>
            </div>
        </div>
    </div>

    <div class="variance-meter">
        <div class="variance-label">Variance</div>
        <div class="variance-bar">
            <div class="variance-fill low" id="variance-fill" style="width: 20%"></div>
        </div>
        <div class="variance-value" id="variance-value">Low</div>
    </div>

    <div class="rollout-container">
        <div class="rollout" id="rollout">
            <!-- Generated by JavaScript -->
        </div>
    </div>

    <div class="computation-rows" id="computation-rows">
        <!-- Generated by JavaScript -->
    </div>

    <div id="product-display" style="display: none;">
        <h4 style="font-size: 0.9rem; color: var(--text-muted); margin-bottom: 8px;">IS Product for Target Computation</h4>
        <div class="product-chain" id="product-chain">
            <!-- Generated by JavaScript -->
        </div>
    </div>

    <div class="step-log" id="step-log">
        <div class="step-log-title">Computation Steps</div>
        <div id="log-entries">
        </div>
    </div>
</section>

<section id="comparison">
    <h2>Bias and Variance</h2>
    <p>
        These two properties govern the quality of any value estimate. <strong>Bias</strong> is systematic error: if our value function \(V\) is inaccurate, any estimate that relies on it inherits that inaccuracy. <strong>Variance</strong> is noise across different samples: even if an estimate is correct on average, individual samples might swing wildly, making each gradient update unreliable.
    </p>
    <p>
        Ideally we want both low bias and low variance. In practice, there is a trade-off. Using more real rewards reduces bias (less reliance on \(V\)) but increases variance (more randomness from the environment and policy). Off-policy IS corrections make this worse by multiplying ratios together.
    </p>

    <table class="comparison-table">
        <thead>
            <tr>
                <th>Method</th>
                <th>Bias</th>
                <th>Variance</th>
                <th>Convergence</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>N-Step IS</strong></td>
                <td><span class="badge badge-low">Low</span></td>
                <td><span class="badge badge-high">Explosive</span></td>
                <td>Can diverge</td>
            </tr>
            <tr>
                <td><strong>Retrace(λ)</strong></td>
                <td><span class="badge badge-low">Low</span></td>
                <td><span class="badge badge-low">Bounded</span></td>
                <td>Guaranteed</td>
            </tr>
            <tr>
                <td><strong>V-trace</strong></td>
                <td><span class="badge badge-low">Low*</span></td>
                <td><span class="badge badge-low">Bounded</span></td>
                <td>Guaranteed</td>
            </tr>
        </tbody>
    </table>
    <p style="font-size: 0.85rem; color: var(--text-muted); margin-top: var(--spacing-sm);">
        *With finite \(\bar{\rho}\), V-trace converges to a policy between \(\mu\) and \(\pi\), introducing slight bias relative to the true target policy.
    </p>

    <div class="lambda-spectrum">
        <div class="spectrum-end left">
            <h4>N-Step IS</h4>
            <div class="formula">ρ₀ × ρ₁ × ... × ρₙ</div>
            <p>Low bias but<br><span class="badge badge-high">Exponential variance</span></p>
        </div>
        <div class="spectrum-middle">
            <div class="spectrum-arrow">⟷</div>
            <p style="margin: 8px 0 0; font-size: 0.85rem;">Retrace(λ)</p>
        </div>
        <div class="spectrum-end right">
            <h4>Truncated IS</h4>
            <div class="formula">min(1,ρ₀) × min(1,ρ₁) × ...</div>
            <p>Low bias with<br><span class="badge badge-low">Bounded variance</span></p>
        </div>
    </div>
</section>

<section id="implementation">
    <h2>Implementation</h2>
    <p>
        Here's a practical implementation of Retrace in Python:
    </p>
    <pre><code><span class="code-keyword">def</span> compute_retrace(rewards, q_values, values,
                    target_probs, behavior_probs,
                    gamma, lambda_):
    <span class="code-comment">"""
    Compute Retrace targets for off-policy learning.

    Args:
        rewards: [T] rewards r_0 to r_{T-1}
        q_values: [T+1] Q(s_t, a_t) values
        values: [T+1] V(s_t) = E[Q(s_t, a)] values
        target_probs: [T] π(a_t|s_t) probabilities
        behavior_probs: [T] μ(a_t|s_t) probabilities
        gamma: discount factor
        lambda_: trace decay parameter
    """</span>
    T = len(rewards)

    <span class="code-comment"># Compute importance sampling ratios</span>
    rhos = target_probs / behavior_probs

    <span class="code-comment"># Truncate ratios (the key Retrace insight)</span>
    cs = lambda_ * np.minimum(<span class="code-number">1.0</span>, rhos)

    <span class="code-comment"># Compute TD errors</span>
    deltas = rewards + gamma * values[<span class="code-number">1</span>:] - q_values[:-<span class="code-number">1</span>]

    <span class="code-comment"># Backward pass to compute targets</span>
    targets = np.zeros(T)
    ret = <span class="code-number">0.0</span>

    <span class="code-keyword">for</span> t <span class="code-keyword">in</span> reversed(range(T)):
        <span class="code-comment"># Accumulate trace-weighted TD errors</span>
        ret = deltas[t] + gamma * cs[t] * ret
        targets[t] = q_values[t] + ret

    <span class="code-keyword">return</span> targets</code></pre>
</section>

<section id="intuition">
    <h2>Why Truncation Works</h2>
    <p>
        The variance problem in n-step IS comes from <em>upweighting</em>. When the target policy strongly prefers an action (ρ >> 1), n-step IS massively amplifies that transition. But we only observed it once. Amplifying a single observation doesn't give us more information, it just adds noise.
    </p>
    <p>
        Retrace's truncation (\(\min(1, \rho)\)) takes an asymmetric approach to fix this:
    </p>
    <ul>
        <li><strong>When ρ < 1</strong> (target policy dislikes this action): the ratio passes through unchanged, correctly reducing this transition's influence.</li>
        <li><strong>When ρ > 1</strong> (target policy prefers this action): the ratio is capped at 1, leaving the transition at its natural observed weight rather than inflating it.</li>
    </ul>

    <div class="insight-box">
        <h4>The Key Insight</h4>
        <p>
            Retrace says: trust the data you have. If the behavior policy took an action the target policy would avoid, downweight it. But if the behavior policy took an action the target policy likes, just use it as-is. Never amplify beyond what was actually observed.
        </p>
    </div>

    <p>
        Because every trace coefficient satisfies \(c_t \leq \lambda \leq 1\), the product of coefficients across multiple steps can only shrink. This is what bounds the variance: no matter how different π and μ are, the correction weights stay controlled.
    </p>

    <h3>Special Cases</h3>
    <p>Retrace smoothly interpolates between familiar algorithms depending on the setting:</p>
    <ul>
        <li><strong>On-policy (π = μ):</strong> All ρ = 1, so c = λ, and Retrace reduces to TD(λ)</li>
        <li><strong>λ = 0:</strong> All traces are zeroed out, giving one-step TD (maximum bias, minimum variance)</li>
        <li><strong>λ = 1, π = μ:</strong> Equivalent to Monte Carlo returns (minimum bias, maximum variance)</li>
    </ul>
</section>

<section id="vtrace">
    <h2>V-trace: Scaling to Distributed Systems</h2>
    <p>
        V-trace (Espeholt et al., 2018) was developed for IMPALA, a massively distributed RL architecture where dozens of actors collect experience in parallel while a central learner updates the policy. In this setting, the behavior policy can be many updates behind the learner, making off-policy correction essential.
    </p>
    <p>
        V-trace is closely related to Retrace but introduces a key structural change: it uses <strong>two separate clipping constants</strong> instead of one. The V-trace target is:
    </p>
    <div class="math-block highlight">
        $$v_s = V(s_s) + \sum_{t=s}^{s+n-1} \gamma^{t-s} \left(\prod_{i=s}^{t-1} \bar{c}_i\right) \bar{\rho}_t \, \delta_t$$
    </div>
    <p>where the two clipping levels serve distinct roles:</p>
    <ul>
        <li>
            \(\bar{\rho}_t = \min(\bar{\rho}, \rho_t)\) clips the importance weight applied to each <strong>TD error</strong>. This controls what value function V-trace converges to. With \(\bar{\rho} = \infty\) it converges to \(V^\pi\); with finite \(\bar{\rho}\) it converges to a policy between \(\mu\) and \(\pi\).
        </li>
        <li>
            \(\bar{c}_t = \min(\bar{c}, \rho_t)\) clips the <strong>trace coefficient</strong> that determines how far corrections propagate backward. This is analogous to Retrace's \(c_t\) and controls the speed of convergence.
        </li>
    </ul>

    <h3>Relationship to Retrace</h3>
    <p>
        When \(\bar{\rho} = \bar{c} = 1\) and \(\lambda = 1\), V-trace and Retrace(1) produce the same targets. The key difference is that V-trace separates the two roles that clipping plays:
    </p>
    <ul>
        <li><strong>Retrace</strong> uses a single clip: \(c_t = \lambda \min(1, \rho_t)\) appears both as the trace coefficient and implicitly as the TD error weight.</li>
        <li><strong>V-trace</strong> decouples these: \(\bar{\rho}\) controls the fixed point while \(\bar{c}\) controls the trace length, and \(\bar{c} \leq \bar{\rho}\) is required.</li>
    </ul>
    <p>
        This decoupling gives V-trace an extra knob. In highly distributed settings where the behavior policy can be very stale, using a moderate \(\bar{\rho}\) sacrifices convergence to the exact target policy in exchange for greater stability. In practice, the IMPALA paper found that \(\bar{\rho} = 1\) and \(\bar{c} = 1\) (equivalent to Retrace) works well, but having the option to tune \(\bar{\rho}\) independently is valuable when policies diverge significantly.
    </p>

    <div class="insight-box">
        <h4>When to use which?</h4>
        <p>
            Use <strong>Retrace</strong> when you need guaranteed convergence to \(Q^\pi\) and the behavior policy is reasonably close. Use <strong>V-trace</strong> in distributed systems where actors may be many updates behind the learner and you need the extra stability from separate \(\bar{\rho}\) clipping.
        </p>
    </div>
</section>

<section id="references">
    <h2>References</h2>
    <p>
        Munos, R., Stepleton, T., Harutyunyan, A., & Bellemare, M. G. (2016). <em>Safe and Efficient Off-Policy Reinforcement Learning.</em> NeurIPS 2016.
    </p>
    <p>
        Espeholt, L., Soyer, H., Munos, R., et al. (2018). <em>IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures.</em> ICML 2018.
    </p>
    <p>
        Precup, D., Sutton, R. S., & Singh, S. (2000). <em>Eligibility Traces for Off-Policy Policy Evaluation.</em> ICML 2000.
    </p>
</section>

<script>
    // Configuration for different divergence levels
    const divergenceConfigs = {
        0: { // Low divergence (similar policies)
            name: 'Low',
            targetProbs: [0.7, 0.6, 0.5, 0.6, 0.7],
            behaviorProbs: [0.6, 0.5, 0.5, 0.5, 0.6]
        },
        1: { // Medium divergence
            name: 'Medium',
            targetProbs: [0.8, 0.3, 0.7, 0.4, 0.9],
            behaviorProbs: [0.4, 0.6, 0.3, 0.7, 0.3]
        },
        2: { // High divergence (very different policies)
            name: 'High',
            targetProbs: [0.9, 0.1, 0.85, 0.15, 0.95],
            behaviorProbs: [0.2, 0.8, 0.2, 0.9, 0.15]
        }
    };

    // Rollout data
    const baseData = {
        states: ['s₀', 's₁', 's₂', 's₃', 's₄', 's₅'],
        qValues: [2.5, 3.1, 2.8, 4.2, 3.0, 0],
        values: [2.8, 3.0, 3.2, 3.8, 2.5, 0],
        rewards: [1.0, -0.5, 2.0, 0.5, -1.0],
        actions: ['a₁', 'a₀', 'a₁', 'a₀', 'a₁']
    };

    let gamma = 0.99;
    let lambda = 0.95;
    let divergence = 1;
    let currentAlgo = 'td';
    let animationInProgress = false;

    // DOM elements
    const gammaSlider = document.getElementById('gamma-slider');
    const lambdaSlider = document.getElementById('lambda-slider');
    const divergenceSlider = document.getElementById('divergence-slider');
    const gammaDisplay = document.getElementById('gamma-display');
    const lambdaDisplay = document.getElementById('lambda-display');
    const divergenceDisplay = document.getElementById('divergence-display');
    const rolloutContainer = document.getElementById('rollout');
    const computationRows = document.getElementById('computation-rows');
    const logEntries = document.getElementById('log-entries');
    const animateBtn = document.getElementById('animate-btn');
    const resetBtn = document.getElementById('reset-btn');
    const lambdaControl = document.getElementById('lambda-control');
    const varianceFill = document.getElementById('variance-fill');
    const varianceValue = document.getElementById('variance-value');
    const productDisplay = document.getElementById('product-display');
    const productChain = document.getElementById('product-chain');
    const algoTabs = document.querySelectorAll('.algorithm-tab');

    function getCurrentConfig() {
        return divergenceConfigs[divergence];
    }

    function computeRatios() {
        const config = getCurrentConfig();
        return config.targetProbs.map((tp, i) => tp / config.behaviorProbs[i]);
    }

    function computeTruncatedRatios() {
        const rhos = computeRatios();
        return rhos.map(rho => lambda * Math.min(1, rho));
    }

    // V-trace uses two separate clipping constants
    const rhoBar = 1;  // clips TD error weight (controls fixed point)
    const cBar = 1;    // clips trace coefficient (controls propagation)

    function computeVtraceRhoBar() {
        const rhos = computeRatios();
        return rhos.map(rho => Math.min(rhoBar, rho));
    }

    function computeVtraceCBar() {
        const rhos = computeRatios();
        return rhos.map(rho => Math.min(cBar, rho));
    }

    function computeTDErrors() {
        const deltas = [];
        for (let t = 0; t < baseData.rewards.length; t++) {
            const delta = baseData.rewards[t] + gamma * baseData.values[t + 1] - baseData.qValues[t];
            deltas.push(delta);
        }
        return deltas;
    }

    function computeTargets() {
        const deltas = computeTDErrors();
        const rhos = computeRatios();
        const cs = computeTruncatedRatios();
        const T = deltas.length;

        if (currentAlgo === 'td') {
            // 1-step TD: just r + γV(s')
            return deltas.map((d, t) => baseData.qValues[t] + d);
        } else if (currentAlgo === 'nstep') {
            // N-step with full IS
            const targets = [];
            for (let t = 0; t < T; t++) {
                let target = 0;
                let isProduct = 1;
                for (let k = t; k < T; k++) {
                    if (k > t) isProduct *= rhos[k];
                    target += isProduct * Math.pow(gamma, k - t) * deltas[k];
                }
                targets.push(baseData.qValues[t] + target);
            }
            return targets;
        } else if (currentAlgo === 'vtrace') {
            // V-trace: separate rho_bar (TD weight) and c_bar (trace)
            const rhoBarVals = computeVtraceRhoBar();
            const cBarVals = computeVtraceCBar();
            const targets = new Array(T).fill(0);
            let ret = 0;
            for (let t = T - 1; t >= 0; t--) {
                const weightedDelta = rhoBarVals[t] * deltas[t];
                ret = weightedDelta + gamma * cBarVals[t] * ret;
                targets[t] = baseData.values[t] + ret;
            }
            return targets;
        } else {
            // Retrace
            const targets = new Array(T).fill(0);
            let ret = 0;
            for (let t = T - 1; t >= 0; t--) {
                ret = deltas[t] + gamma * cs[t] * ret;
                targets[t] = baseData.qValues[t] + ret;
            }
            return targets;
        }
    }

    function computeVariance() {
        if (currentAlgo === 'td') return { level: 'low', percent: 15, label: 'Low' };

        const rhos = computeRatios();

        if (currentAlgo === 'nstep') {
            const product = rhos.reduce((a, b) => a * b, 1);
            if (product > 10) return { level: 'high', percent: 95, label: 'Explosive!' };
            if (product > 3) return { level: 'high', percent: 80, label: 'Very High' };
            if (product > 1.5) return { level: 'medium', percent: 60, label: 'High' };
            return { level: 'medium', percent: 40, label: 'Moderate' };
        } else if (currentAlgo === 'vtrace') {
            const cBarVals = computeVtraceCBar();
            const maxTrace = cBarVals.reduce((a, b) => a * b, 1);
            if (maxTrace > 0.5) return { level: 'low', percent: 30, label: 'Low' };
            return { level: 'low', percent: 20, label: 'Very Low' };
        } else {
            const cs = computeTruncatedRatios();
            const maxTrace = cs.reduce((a, b) => a * b, 1);
            if (maxTrace > 0.5) return { level: 'low', percent: 30, label: 'Low' };
            return { level: 'low', percent: 20, label: 'Very Low' };
        }
    }

    function updateVarianceDisplay() {
        const variance = computeVariance();
        varianceFill.className = `variance-fill ${variance.level}`;
        varianceFill.style.width = `${variance.percent}%`;
        varianceValue.textContent = variance.label;
        varianceValue.style.color = variance.level === 'high' ? 'var(--accent-red)' :
                                    variance.level === 'medium' ? 'var(--accent-orange)' :
                                    'var(--accent-green)';
    }

    function renderRollout() {
        const config = getCurrentConfig();
        const rhos = computeRatios();
        let html = '';

        for (let t = 0; t < baseData.states.length; t++) {
            html += `
                <div class="timestep" data-t="${t}">
                    <div class="state-node" id="state-${t}">${baseData.states[t]}</div>
                    <div class="value-label">Q(s,a)</div>
                    <div class="value">${baseData.qValues[t].toFixed(1)}</div>
                    <div class="value-label" style="margin-top: 4px;">V(s)</div>
                    <div class="value" style="color: var(--text-secondary);">${baseData.values[t].toFixed(1)}</div>
                </div>
            `;

            if (t < baseData.rewards.length) {
                const reward = baseData.rewards[t];
                const rewardClass = reward >= 0 ? 'positive' : 'negative';
                const rho = rhos[t];
                const rhoClass = rho > 1.5 ? 'high' : rho < 0.7 ? 'low' : 'normal';

                html += `
                    <div class="transition-extended">
                        <div class="arrow"></div>
                        <div class="reward ${rewardClass}">${reward >= 0 ? '+' : ''}${reward.toFixed(1)}</div>
                        <div class="reward-label">r${t}</div>
                        <div class="action-label">${baseData.actions[t]}</div>
                        <div class="ratio-display" style="margin-top: 12px;">
                            <div class="ratio-fraction">
                                <span class="ratio-numerator">π=${config.targetProbs[t].toFixed(1)}</span>
                                <span class="ratio-denominator">μ=${config.behaviorProbs[t].toFixed(1)}</span>
                            </div>
                            <div class="ratio-value ${rhoClass}" id="rho-${t}">ρ=${rho.toFixed(2)}</div>
                        </div>
                    </div>
                `;
            }
        }

        rolloutContainer.innerHTML = html;
    }

    function renderComputationRows() {
        const deltas = computeTDErrors();
        const rhos = computeRatios();
        const cs = computeTruncatedRatios();
        const targets = computeTargets();

        let html = '';

        // TD errors row
        html += '<div class="computation-row"><div class="label">δ (TD)</div>';
        for (let t = 0; t < deltas.length; t++) {
            html += `
                <div class="computation-cell">
                    <span class="symbol">δ${t}</span>
                    <span class="value td visible" id="td-${t}">${deltas[t].toFixed(3)}</span>
                </div>
            `;
            if (t < deltas.length - 1) html += '<div class="spacer-cell"></div>';
        }
        html += '</div>';

        // Show IS ratios row for n-step and retrace
        if (currentAlgo !== 'td') {
            html += '<div class="trace-row"><div class="label" style="min-width: 60px;">ρ (IS)</div>';
            for (let t = 0; t < rhos.length; t++) {
                const rhoClass = rhos[t] > 1.5 ? 'full' : 'truncated';
                html += `
                    <div class="trace-cell">
                        <span class="trace-symbol">ρ${t}</span>
                        <span class="trace-value ${rhoClass}" id="rho-val-${t}">${rhos[t].toFixed(2)}</span>
                    </div>
                `;
                if (t < rhos.length - 1) html += '<div class="spacer-cell"></div>';
            }
            html += '</div>';
        }

        // Show truncated coefficients for retrace
        if (currentAlgo === 'retrace') {
            html += '<div class="trace-row"><div class="label" style="min-width: 60px;">c (trace)</div>';
            for (let t = 0; t < cs.length; t++) {
                const wasTruncated = rhos[t] > 1;
                html += `
                    <div class="trace-cell">
                        <span class="trace-symbol">c${t}</span>
                        <span class="trace-value truncated" id="c-${t}">${cs[t].toFixed(2)}</span>
                        ${wasTruncated ? '<span class="truncate-indicator">capped</span>' : ''}
                    </div>
                `;
                if (t < cs.length - 1) html += '<div class="spacer-cell"></div>';
            }
            html += '</div>';
        }

        // Show V-trace clipped coefficients (rho-bar and c-bar)
        if (currentAlgo === 'vtrace') {
            const rhoBarVals = computeVtraceRhoBar();
            const cBarVals = computeVtraceCBar();
            html += '<div class="trace-row"><div class="label" style="min-width: 60px;">ρ&#x0304; (TD)</div>';
            for (let t = 0; t < rhoBarVals.length; t++) {
                const wasTruncated = rhos[t] > rhoBar;
                html += `
                    <div class="trace-cell">
                        <span class="trace-symbol">ρ&#x0304;${t}</span>
                        <span class="trace-value truncated" id="rhobar-${t}">${rhoBarVals[t].toFixed(2)}</span>
                        ${wasTruncated ? '<span class="truncate-indicator">capped</span>' : ''}
                    </div>
                `;
                if (t < rhoBarVals.length - 1) html += '<div class="spacer-cell"></div>';
            }
            html += '</div>';
            html += '<div class="trace-row"><div class="label" style="min-width: 60px;">c&#x0304; (trace)</div>';
            for (let t = 0; t < cBarVals.length; t++) {
                const wasTruncated = rhos[t] > cBar;
                html += `
                    <div class="trace-cell">
                        <span class="trace-symbol">c&#x0304;${t}</span>
                        <span class="trace-value truncated" id="cbar-${t}">${cBarVals[t].toFixed(2)}</span>
                        ${wasTruncated ? '<span class="truncate-indicator">capped</span>' : ''}
                    </div>
                `;
                if (t < cBarVals.length - 1) html += '<div class="spacer-cell"></div>';
            }
            html += '</div>';
        }

        // Target row
        const targetLabel = currentAlgo === 'td' ? 'G (1-step)' :
                          currentAlgo === 'nstep' ? 'G (n-step IS)' :
                          currentAlgo === 'vtrace' ? 'v (V-trace)' : 'G (retrace)';
        html += `<div class="computation-row"><div class="label">${targetLabel}</div>`;
        for (let t = 0; t < targets.length; t++) {
            const gaeClass = targets[t] >= baseData.qValues[t] ? 'gae-positive' : 'gae-negative';
            html += `
                <div class="computation-cell">
                    <span class="symbol">G${t}</span>
                    <span class="value ${gaeClass} visible" id="target-${t}">${targets[t].toFixed(3)}</span>
                </div>
            `;
            if (t < targets.length - 1) html += '<div class="spacer-cell"></div>';
        }
        html += '</div>';

        computationRows.innerHTML = html;
    }

    function renderProductChain(upToIndex = -1) {
        if (currentAlgo === 'td') {
            productDisplay.style.display = 'none';
            return;
        }

        productDisplay.style.display = 'block';
        const rhos = computeRatios();
        const cs = computeTruncatedRatios();
        const useCs = currentAlgo === 'retrace' || currentAlgo === 'vtrace';
        const vals = currentAlgo === 'vtrace' ? computeVtraceCBar() : useCs ? cs : rhos;

        let html = '';
        let product = 1;

        for (let i = 0; i <= Math.min(upToIndex, vals.length - 1); i++) {
            if (i > 0) html += '<span class="product-operator">×</span>';
            const included = upToIndex < 0 || i <= upToIndex;
            const val = vals[i];
            product *= val;

            const label = currentAlgo === 'vtrace' ? `c\u0304${i}` : useCs ? `c${i}` : `ρ${i}`;
            html += `<span class="product-term ${included ? 'included' : 'excluded'}">${label}=${val.toFixed(2)}</span>`;
        }

        if (upToIndex >= 0) {
            html += `<span class="product-result">= ${product.toFixed(3)}</span>`;
        }

        productChain.innerHTML = html;
    }

    function updateLog() {
        let html = '';

        if (currentAlgo === 'td') {
            html = `
                <div class="log-entry visible">
                    <span class="log-step">Formula:</span>
                    <span class="log-formula">Gₜ = rₜ + γV(sₜ₊₁)</span>
                </div>
                <div class="log-entry visible">
                    <span class="log-step">Note:</span>
                    <span class="log-formula">No IS correction needed for 1-step</span>
                </div>
            `;
        } else if (currentAlgo === 'nstep') {
            html = `
                <div class="log-entry visible">
                    <span class="log-step">Formula:</span>
                    <span class="log-formula">Gₜ = (ρₜ×ρₜ₊₁×...×ρₙ) × (Σγᵏrₜ₊ₖ + γⁿV(sₙ))</span>
                </div>
                <div class="log-entry visible">
                    <span class="log-step">Warning:</span>
                    <span class="log-formula" style="color: var(--accent-red);">Product of IS ratios can explode!</span>
                </div>
            `;
        } else if (currentAlgo === 'vtrace') {
            html = `
                <div class="log-entry visible">
                    <span class="log-step">Step 1:</span>
                    <span class="log-formula">Compute ρ̄ₜ = min(ρ̄, ρₜ) and c̄ₜ = min(c̄, ρₜ)</span>
                </div>
                <div class="log-entry visible">
                    <span class="log-step">Step 2:</span>
                    <span class="log-formula">Backward: vₛ = V(sₛ) + Σ γᵗ⁻ˢ (∏c̄ᵢ) ρ̄ₜδₜ</span>
                </div>
                <div class="log-entry visible">
                    <span class="log-step">Key:</span>
                    <span class="log-formula" style="color: var(--accent-green);">Two clips: ρ̄ controls fixed point, c̄ controls trace</span>
                </div>
            `;
        } else {
            html = `
                <div class="log-entry visible">
                    <span class="log-step">Step 1:</span>
                    <span class="log-formula">Compute cₜ = λ × min(1, ρₜ)</span>
                </div>
                <div class="log-entry visible">
                    <span class="log-step">Step 2:</span>
                    <span class="log-formula">Backward: Gₜ = rₜ + γ[cₜ₊₁(Gₜ₊₁ - Qₜ₊₁) + Vₜ₊₁]</span>
                </div>
                <div class="log-entry visible">
                    <span class="log-step">Key:</span>
                    <span class="log-formula" style="color: var(--accent-green);">Truncation prevents variance explosion</span>
                </div>
            `;
        }

        logEntries.innerHTML = html;
    }

    async function animate() {
        if (animationInProgress) return;
        animationInProgress = true;
        animateBtn.disabled = true;
        resetBtn.disabled = true;

        const deltas = computeTDErrors();
        const rhos = computeRatios();
        const cs = computeTruncatedRatios();
        const T = deltas.length;

        // Reset visibility
        document.querySelectorAll('.computation-cell .value').forEach(el => {
            el.classList.remove('visible');
        });
        document.querySelectorAll('.state-node').forEach(el => {
            el.classList.remove('active', 'computed');
        });

        // Animate based on algorithm
        if (currentAlgo === 'td') {
            // Forward pass for TD
            for (let t = 0; t < T; t++) {
                document.getElementById(`state-${t}`).classList.add('active');
                await sleep(200);

                document.getElementById(`td-${t}`).classList.add('visible');
                await sleep(200);

                document.getElementById(`target-${t}`).classList.add('visible');
                document.getElementById(`state-${t}`).classList.remove('active');
                document.getElementById(`state-${t}`).classList.add('computed');
                await sleep(300);
            }
        } else {
            // Show TD errors first
            for (let t = 0; t < T; t++) {
                document.getElementById(`td-${t}`).classList.add('visible');
                await sleep(100);
            }
            await sleep(300);

            // Show ratios
            for (let t = 0; t < T; t++) {
                const rhoEl = document.getElementById(`rho-val-${t}`);
                if (rhoEl) rhoEl.style.opacity = '1';
                await sleep(100);
            }
            await sleep(300);

            if (currentAlgo === 'retrace' || currentAlgo === 'vtrace') {
                // Show truncated/clipped coefficients
                for (let t = 0; t < T; t++) {
                    if (currentAlgo === 'retrace') {
                        const cEl = document.getElementById(`c-${t}`);
                        if (cEl) cEl.style.opacity = '1';
                    } else {
                        const rhoBarEl = document.getElementById(`rhobar-${t}`);
                        const cBarEl = document.getElementById(`cbar-${t}`);
                        if (rhoBarEl) rhoBarEl.style.opacity = '1';
                        if (cBarEl) cBarEl.style.opacity = '1';
                    }
                    await sleep(100);
                }
                await sleep(300);

                // Backward pass
                for (let t = T - 1; t >= 0; t--) {
                    document.getElementById(`state-${t}`).classList.add('active');
                    renderProductChain(T - 1 - t);
                    await sleep(300);

                    document.getElementById(`target-${t}`).classList.add('visible');
                    document.getElementById(`state-${t}`).classList.remove('active');
                    document.getElementById(`state-${t}`).classList.add('computed');
                    await sleep(300);
                }
            } else {
                // N-step: show product building up
                for (let t = 0; t < T; t++) {
                    document.getElementById(`state-${t}`).classList.add('active');
                    renderProductChain(t);
                    await sleep(400);

                    document.getElementById(`target-${t}`).classList.add('visible');
                    document.getElementById(`state-${t}`).classList.remove('active');
                    document.getElementById(`state-${t}`).classList.add('computed');
                    await sleep(300);
                }
            }
        }

        animationInProgress = false;
        animateBtn.disabled = false;
        resetBtn.disabled = false;
    }

    function reset() {
        renderRollout();
        renderComputationRows();
        updateVarianceDisplay();
        updateLog();
        renderProductChain(-1);

        // Show lambda control only for retrace (V-trace doesn't use lambda)
        lambdaControl.style.display = currentAlgo === 'retrace' ? 'block' : 'none';
    }

    function sleep(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }

    // Event listeners
    gammaSlider.addEventListener('input', (e) => {
        gamma = parseFloat(e.target.value);
        gammaDisplay.textContent = gamma.toFixed(2);
        reset();
    });

    lambdaSlider.addEventListener('input', (e) => {
        lambda = parseFloat(e.target.value);
        lambdaDisplay.textContent = lambda.toFixed(2);
        reset();
    });

    divergenceSlider.addEventListener('input', (e) => {
        divergence = parseInt(e.target.value);
        divergenceDisplay.textContent = getCurrentConfig().name;
        reset();
    });

    algoTabs.forEach(tab => {
        tab.addEventListener('click', () => {
            algoTabs.forEach(t => t.classList.remove('active'));
            tab.classList.add('active');
            currentAlgo = tab.dataset.algo;
            reset();
        });
    });

    animateBtn.addEventListener('click', animate);
    resetBtn.addEventListener('click', reset);

    // Initial render
    reset();
</script>
